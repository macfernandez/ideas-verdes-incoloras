{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4034483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar librerías, en caso de que no las tengan instaladas.\n",
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a6880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comandos import\n",
    "import fasttext\n",
    "import numpy\n",
    "import gensim\n",
    "import logging\n",
    "import gensim.downloader as api\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import os\n",
    "from os import listdir\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024edb6d",
   "metadata": {},
   "source": [
    "# Algunas nociones básicas\n",
    "\n",
    "**Hipótesis distribucional**: Las palabras que aparecen en los mismos contextos tienen significados similares.\n",
    "\n",
    "**Semántica de vectores**: Semántica que implementa la hipótesis distribucional mediante un modelo de aprendizaje automático que construye representaciones del significado de las palabras de manera tal que cada palabra es un punto en un espacio multidimensional derivado de la distribución de las palabras vecinas en corpus. Cada punto se define mediante un vector que se denomina embedding.\n",
    "\n",
    "\n",
    "El primer modelo de semántica de vectores surgió en el área de análisis de sentimiento. Cada paalbra se representaba como un vector en un espacio de tres dimensiones: valencia, *arousal*, y dominancia.\n",
    "\n",
    "Los dos modelos más conocidos de semántica de vectores son tf-idf y word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae9381",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1794a8",
   "metadata": {},
   "source": [
    "**matriz de coocurrencia**: modelo vectorial para representar cuan frecuentemente coocurren las palabras. Existen distintos tipos: matriz de términos-documentos, en que las filas son palabras y las columnas son documentos; matriz de término-término, en que cada celda indica la cantidad de veces que coocurren la palabra de la columna con la de la fila.\n",
    "\n",
    "**tf**: frecuencia de un determinado término\n",
    "\n",
    "**df**: cantidad de documentos en que aparece un término.\n",
    "\n",
    "**idf**: número total de documentos dividido la cantidad de documentos en que aparece un término en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca487cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['vegetables are good for health', 'fruits are good too',\n",
    "'vegetables are good for health, fruits are good too and they are healthy']\n",
    "\n",
    "def vectorizacion(corpus, language):\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords.words(language))\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus).toarray()\n",
    "    print('la cantidad de documentos y palabras a considerar son '+str(tfidf_matrix.shape)) # Cantidad de documentos y cantidad de palabras\n",
    "    print('los términos a considerar son '+str(vectorizer.get_feature_names())) # Términos a considerar\n",
    "    doc_number = 0\n",
    "    for file in corpus:\n",
    "        print(tfidf_matrix[doc_number])\n",
    "        doc_number = doc_number + 1\n",
    "    return doc_number, vectorizer\n",
    "    \n",
    "    \n",
    "vectorizacion(corpus,'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcf6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(corpus, query, language):\n",
    "    doc_number, vectorizer = vectorizacion(corpus,language)\n",
    "    print(doc_number)\n",
    "#    print(vectorizer)\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus).toarray()\n",
    "#        vectorizer = TfidfVectorizer(stop_words=language)\n",
    "    new_vector = vectorizer.transform(query).toarray().reshape(-1)\n",
    "    reference_number = 0\n",
    "    while reference_number <= int(doc_number)-1:\n",
    "        print('tfidf'+str(reference_number)+', similarity score: ',spatial.distance.cosine(new_vector,tfidf_matrix[reference_number]))\n",
    "        reference_number = reference_number + 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['fruits and vegetables are good for health']\n",
    "comparison(corpus,query,'english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0084cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función toma un directorio y devuelve una lista que contiene \n",
    "# todo archivo con una extensión particular en ese directorio\n",
    "def upload_files(path, extension):\n",
    "    filenames = listdir(path)\n",
    "    fullnames = [ os.path.join(path, filenames) for filenames in filenames if os.path.join(path, filenames).endswith( extension ) ]\n",
    "    return fullnames\n",
    "\n",
    "# Esta función toma todo txt en un directorio que contiene al corpus  \n",
    "# and returns the list of sentences from that corpus\n",
    "\n",
    "def consider_corpus_files(path):\n",
    "    sent_list = []\n",
    "    corpus = []\n",
    "    for file in upload_files(path, \"txt\"):\n",
    "        fi = open(file, \"r\")\n",
    "        corpus.append(fi.read())\n",
    "    return corpus\n",
    "\n",
    "consider_corpus_files('corpus/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662cfd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizacion(consider_corpus_files('corpus/'),'spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157761dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['facundo y rosas']\n",
    "comparison(consider_corpus_files('corpus/'),query,'spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84128de9",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd7a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93909974",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_car = wv['car']\n",
    "print(vec_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2aae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = wv['king']\n",
    "print(vec_king)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae86cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_male = wv['male']\n",
    "print(vec_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4bc506",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_woman = wv['woman']\n",
    "print(vec_woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f424bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queen = wv['queen']\n",
    "print(vec_queen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy = vec_king - vec_male + vec_woman\n",
    "print(analogy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0dbbbe",
   "metadata": {},
   "source": [
    "Calcular similitud mediante cálculo del coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033bb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coseno entre king y queen y king y male usando gensim\n",
    "pairs = [\n",
    "    ('king', 'queen'),\n",
    "    ('king', 'male'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc65199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coseno entre king y queen usando scipy\n",
    "result = 1 - spatial.distance.cosine(vec_king, vec_queen)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc78ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coseno entre queen y queen-male+woman usando scipy\n",
    "result = 1 - spatial.distance.cosine(vec_queen, analogy)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coseno entre king y queen-male+woman usando scipy\n",
    "result = 1 - spatial.distance.cosine(vec_king, analogy)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97834a35",
   "metadata": {},
   "source": [
    "Se puede calcular las 5 palabras más cercanas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cea614",
   "metadata": {},
   "source": [
    "El modelo no es capaz de inferir un vector para una palabra desconocida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a5b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    word = 'cameroon'\n",
    "    vec_cameroon = wv[word]\n",
    "except KeyError:\n",
    "    print(\"The word \"+word+\" does not appear in this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c6f5d",
   "metadata": {},
   "source": [
    "Entrenar un modelo propio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = open('corpus/facundo.txt')\n",
    "        for line in corpus_path:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3034066",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbeef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_Facundo = model.wv['facundo']\n",
    "print(vec_Facundo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['rosas'], topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc76a7",
   "metadata": {},
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910571c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "# Baja el modelo y lo divide\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz && tar xvzf cooking.stackexchange.tar.gz\n",
    "!head cooking.stackexchange.txt\n",
    "!head -n 12404 cooking.stackexchange.txt > cooking.train\n",
    "!tail -n 3000 cooking.stackexchange.txt > cooking.valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecccfdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"cooking.train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4376382",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\"Which baking dish is best to bake a banana bread ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99728ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\"Which fruit is sweet ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e206a8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
